{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define alguns parÃ¢metros para ambos os classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = \"google-bert/bert-base-cased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceamento da classe na dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(X, y):\n",
    "    X_train_bal = X.copy()\n",
    "    y_train_bal = y.copy()\n",
    "    count_class = pd.DataFrame(y.value_counts()).reset_index()\n",
    "    max_class_count = count_class.iloc[0]['count']\n",
    "    count_class = count_class.iloc[1:]\n",
    "    for _, row in count_class.iterrows():\n",
    "        sample = y[y == row['class']].sample(max_class_count - row['count'], replace=True)\n",
    "        X_train_bal = pd.concat([X_train_bal, X_train[sample.index]])\n",
    "        y_train_bal = pd.concat([y_train_bal, sample])\n",
    "\n",
    "    return (X_train_bal, y_train_bal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstr = pd.read_csv('../dados/atv3/CSTR.csv')\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(cstr['text'], cstr['class'], test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstr_lb = LabelEncoder().fit(cstr['class'])\n",
    "num_labels = len(cstr_lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 3)\n",
      "(209,)\n",
      "(30,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "print(cstr.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Robotics                   85\n",
       "ArtificiallIntelligence    85\n",
       "Theory                     85\n",
       "Systems                    85\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal, y_train_bal = balance_df(X_train, y_train)\n",
    "y_train_bal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cstr = AutoTokenizer.from_pretrained(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'un',\n",
       " '##iba',\n",
       " '##nco',\n",
       " '##rp',\n",
       " 'in',\n",
       " '##c',\n",
       " 'u',\n",
       " '##b',\n",
       " '##c',\n",
       " '##p',\n",
       " 'regular',\n",
       " 'divide',\n",
       " '##nd',\n",
       " 'set',\n",
       " 'q',\n",
       " '##tly',\n",
       " 'di',\n",
       " '##v',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'vs',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'previously',\n",
       " 'pay',\n",
       " 'a',\n",
       " '##p',\n",
       " '##ril',\n",
       " 'record',\n",
       " 'march',\n",
       " 're',\n",
       " '##uter',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_cstr.tokenize(X_train_bal.iloc[0], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer_cstr(examples, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = [dict(tokenize_function(X_train_bal.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_train_bal))]\n",
    "val_ds = [dict(tokenize_function(X_val.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_val))]\n",
    "test_ds = [dict(tokenize_function(X_test.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_cstr = AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=num_labels, torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "logging_step = len(train_ds)//batch_size\n",
    "model_ckpt = str(datetime.datetime.today().date().isoformat())\n",
    "model_name = f\"{model_ckpt}-finetune-cstr\"\n",
    "model_name_final = f\"{model_ckpt}-bert-finetune-cstr-final\"\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvanerven/venv/ppgi/atv1/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_step,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\",\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_cstr,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset=val_ds,\n",
    "                  processing_class=tokenizer_cstr\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 01:50, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.471900</td>\n",
       "      <td>0.680709</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.595413</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.786457</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=86, training_loss=0.3106432681859926, metrics={'train_runtime': 111.598, 'train_samples_per_second': 6.093, 'train_steps_per_second': 0.771, 'total_flos': 178918730465280.0, 'train_loss': 0.3106432681859926, 'epoch': 2.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_name_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8333333333333334,\n",
       " 'f1_macro': np.float64(0.8289159341897678),\n",
       " 'f1_micro': np.float64(0.8333333333333334)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(trainer.predict(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "re8 = pd.read_csv('../dados/atv3/re8.csv')\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(re8['text'], re8['class'], test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re8_lb = LabelEncoder().fit(re8['class'])\n",
    "num_labels = len(re8_lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7674, 3)\n",
      "(5371,)\n",
      "(783,)\n",
      "(1520,)\n"
     ]
    }
   ],
   "source": [
    "print(re8.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "earn        2771\n",
       "money       2771\n",
       "interest    2771\n",
       "ship        2771\n",
       "acq         2771\n",
       "trade       2771\n",
       "crude       2771\n",
       "grain       2771\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal, y_train_bal = balance_df(X_train, y_train)\n",
    "y_train_bal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_re8 = AutoTokenizer.from_pretrained(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'un',\n",
       " '##iba',\n",
       " '##nco',\n",
       " '##rp',\n",
       " 'in',\n",
       " '##c',\n",
       " 'u',\n",
       " '##b',\n",
       " '##c',\n",
       " '##p',\n",
       " 'regular',\n",
       " 'divide',\n",
       " '##nd',\n",
       " 'set',\n",
       " 'q',\n",
       " '##tly',\n",
       " 'di',\n",
       " '##v',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'vs',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'previously',\n",
       " 'pay',\n",
       " 'a',\n",
       " '##p',\n",
       " '##ril',\n",
       " 'record',\n",
       " 'march',\n",
       " 're',\n",
       " '##uter',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_re8.tokenize(X_train_bal.iloc[0], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer_re8(examples, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = [dict(tokenize_function(X_train_bal.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_train_bal))]\n",
    "val_ds = [dict(tokenize_function(X_val.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_val))]\n",
    "test_ds = [dict(tokenize_function(X_test.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_re8 = AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=num_labels, torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "logging_step = len(train_ds)//batch_size\n",
    "model_ckpt = str(datetime.datetime.today().date().isoformat())\n",
    "model_name = f\"{model_ckpt}-finetune-re8\"\n",
    "model_name_final = f\"{model_ckpt}-bert-finetune-re8-final\"\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvanerven/venv/ppgi/atv1/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_step,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\",\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_re8,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset=val_ds,\n",
    "                  processing_class=tokenizer_re8\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5542' max='5542' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5542/5542 1:29:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.201838</td>\n",
       "      <td>0.964240</td>\n",
       "      <td>0.906374</td>\n",
       "      <td>0.964240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.096913</td>\n",
       "      <td>0.984674</td>\n",
       "      <td>0.961595</td>\n",
       "      <td>0.984674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5542, training_loss=0.1402223959419969, metrics={'train_runtime': 5354.9255, 'train_samples_per_second': 8.279, 'train_steps_per_second': 1.035, 'total_flos': 1.1665920178126848e+16, 'train_loss': 0.1402223959419969, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_name_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9756578947368421,\n",
       " 'f1_macro': np.float64(0.9496402828135407),\n",
       " 'f1_micro': np.float64(0.9756578947368421)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(trainer.predict(test_ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
