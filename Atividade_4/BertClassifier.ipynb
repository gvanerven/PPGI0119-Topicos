{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define alguns par√¢metros para ambos os classificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = \"google-bert/bert-base-cased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balanceamento da classe no dataset de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df(X, y):\n",
    "    X_train_bal = X.copy()\n",
    "    y_train_bal = y.copy()\n",
    "    count_class = pd.DataFrame(y.value_counts()).reset_index()\n",
    "    max_class_count = count_class.iloc[0]['count']\n",
    "    count_class = count_class.iloc[1:]\n",
    "    for _, row in count_class.iterrows():\n",
    "        sample = y[y == row['class']].sample(max_class_count - row['count'], replace=True)\n",
    "        X_train_bal = pd.concat([X_train_bal, X_train[sample.index]])\n",
    "        y_train_bal = pd.concat([y_train_bal, sample])\n",
    "\n",
    "    return (X_train_bal, y_train_bal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstr = pd.read_csv('../dados/atv3/CSTR.csv')\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(cstr['text'], cstr['class'], test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstr_lb = LabelEncoder().fit(cstr['class'])\n",
    "num_labels = len(cstr_lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(299, 3)\n",
      "(209,)\n",
      "(30,)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "print(cstr.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "Robotics                   85\n",
       "ArtificiallIntelligence    85\n",
       "Theory                     85\n",
       "Systems                    85\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal, y_train_bal = balance_df(X_train, y_train)\n",
    "y_train_bal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_cstr = AutoTokenizer.from_pretrained(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'challenges',\n",
       " 'in',\n",
       " 'systems',\n",
       " 'ne',\n",
       " '##uro',\n",
       " '##science',\n",
       " 'is',\n",
       " 'a',\n",
       " 'satisfactory',\n",
       " 'model',\n",
       " 'of',\n",
       " 'neural',\n",
       " 'signaling',\n",
       " '.',\n",
       " 'From',\n",
       " 'rate',\n",
       " 'coding',\n",
       " 'to',\n",
       " 'temporal',\n",
       " 'coding',\n",
       " ',',\n",
       " 'models',\n",
       " 'of',\n",
       " 'neural',\n",
       " 'signaling',\n",
       " 'have',\n",
       " 'been',\n",
       " 'challenged',\n",
       " 'by',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'neurons',\n",
       " 'fire',\n",
       " 'highly',\n",
       " 'irregular',\n",
       " '##ly',\n",
       " '.',\n",
       " 'A',\n",
       " 'typical',\n",
       " 'interpretation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'var',\n",
       " '##iability',\n",
       " 'is',\n",
       " '`',\n",
       " '`',\n",
       " 'noise',\n",
       " 'other',\n",
       " 'than',\n",
       " 'signal',\n",
       " \"'\",\n",
       " \"'\",\n",
       " ',',\n",
       " 'which',\n",
       " 'not',\n",
       " 'only',\n",
       " 'has',\n",
       " 'difficulty',\n",
       " 'accounting',\n",
       " 'for',\n",
       " 'the',\n",
       " 'speed',\n",
       " ',',\n",
       " 'accuracy',\n",
       " ',',\n",
       " 'efficiency',\n",
       " 'and',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'biological',\n",
       " 'systems',\n",
       " ',',\n",
       " 'but',\n",
       " 'is',\n",
       " 'also',\n",
       " 'con',\n",
       " '##tra',\n",
       " '##dict',\n",
       " '##ed',\n",
       " 'by',\n",
       " 'recent',\n",
       " 'studies',\n",
       " 'that',\n",
       " 'show',\n",
       " 'both',\n",
       " 'spike',\n",
       " 'generation',\n",
       " 'and',\n",
       " 'transmission',\n",
       " 'are',\n",
       " 'highly',\n",
       " 'reliable',\n",
       " '.',\n",
       " 'Challenge',\n",
       " '##d',\n",
       " 'with',\n",
       " 'the',\n",
       " 'disc',\n",
       " '##re',\n",
       " '##pan',\n",
       " '##cy',\n",
       " 'between',\n",
       " 'theory',\n",
       " 'and',\n",
       " 'data',\n",
       " ',',\n",
       " 'we',\n",
       " 'take',\n",
       " 'a',\n",
       " 'fresh',\n",
       " 'view',\n",
       " 'of',\n",
       " 'the',\n",
       " 'subject',\n",
       " 'with',\n",
       " 'the',\n",
       " 'proposal',\n",
       " 'that',\n",
       " 'the',\n",
       " 'random',\n",
       " '##ness',\n",
       " 'associated',\n",
       " 'with',\n",
       " 'ne',\n",
       " '##uron',\n",
       " '##al',\n",
       " 'output',\n",
       " '##s',\n",
       " 'is',\n",
       " 'certain',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'purpose',\n",
       " '.',\n",
       " 'In',\n",
       " 'particular',\n",
       " ',',\n",
       " 'we',\n",
       " 'model',\n",
       " 'neurons',\n",
       " 'as',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bil',\n",
       " '##istic',\n",
       " 'devices',\n",
       " 'that',\n",
       " 'not',\n",
       " 'only',\n",
       " 'com',\n",
       " '##pute',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bilities',\n",
       " 'but',\n",
       " 'also',\n",
       " 'fire',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bil',\n",
       " '##istic',\n",
       " '##ally',\n",
       " 'to',\n",
       " 'signal',\n",
       " 'their',\n",
       " 'com',\n",
       " '##putation',\n",
       " '##s',\n",
       " '.',\n",
       " 'According',\n",
       " 'to',\n",
       " 'our',\n",
       " 'model',\n",
       " ',',\n",
       " 'signaling',\n",
       " 'of',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bilities',\n",
       " 'is',\n",
       " 'done',\n",
       " 'by',\n",
       " 'having',\n",
       " 'cells',\n",
       " 'with',\n",
       " 'similar',\n",
       " 're',\n",
       " '##ceptive',\n",
       " 'fields',\n",
       " 'fire',\n",
       " 's',\n",
       " '##ync',\n",
       " '##hr',\n",
       " '##ono',\n",
       " '##us',\n",
       " '##ly',\n",
       " 'to',\n",
       " 'achieve',\n",
       " 'fast',\n",
       " 'communication',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'observations',\n",
       " 'of',\n",
       " 'neurons',\n",
       " 'coding',\n",
       " 'as',\n",
       " 'ensembles',\n",
       " 'and',\n",
       " 'top',\n",
       " '##ographic',\n",
       " 'map',\n",
       " 'organization',\n",
       " '.',\n",
       " 'Our',\n",
       " 'proposal',\n",
       " 'of',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bil',\n",
       " '##istic',\n",
       " ',',\n",
       " 'distributed',\n",
       " 's',\n",
       " '##ync',\n",
       " '##hr',\n",
       " '##ono',\n",
       " '##us',\n",
       " 'vol',\n",
       " '##ley',\n",
       " '##s',\n",
       " 'as',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'signaling',\n",
       " 'strategy',\n",
       " 'not',\n",
       " 'only',\n",
       " 'accounts',\n",
       " 'for',\n",
       " 'variable',\n",
       " 'neural',\n",
       " 'responses',\n",
       " ',',\n",
       " 'but',\n",
       " 'also',\n",
       " 'provides',\n",
       " 'the',\n",
       " 'advantage',\n",
       " 'of',\n",
       " 'robust',\n",
       " 'and',\n",
       " 'fast',\n",
       " 'com',\n",
       " '##putation',\n",
       " '.',\n",
       " 'Furthermore',\n",
       " ',',\n",
       " 'the',\n",
       " 'involvement',\n",
       " '##s',\n",
       " 'of',\n",
       " 'pro',\n",
       " '##ba',\n",
       " '##bil',\n",
       " '##istic',\n",
       " 'firing',\n",
       " 'and',\n",
       " 'distributed',\n",
       " 'coding',\n",
       " 'ex',\n",
       " '##plicate',\n",
       " 'how',\n",
       " 's',\n",
       " '##ync',\n",
       " '##hr',\n",
       " '##ono',\n",
       " '##us',\n",
       " 'firing',\n",
       " 'can',\n",
       " 'appear',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'rate',\n",
       " 'code',\n",
       " ',',\n",
       " 'accounting',\n",
       " 'for',\n",
       " 'the',\n",
       " 'vast',\n",
       " 'amount',\n",
       " 'of',\n",
       " 'data',\n",
       " 'supporting',\n",
       " 'a',\n",
       " 'rate',\n",
       " 'code',\n",
       " 'assumption',\n",
       " '.',\n",
       " 'Any',\n",
       " 'neural',\n",
       " 'signaling',\n",
       " 'model',\n",
       " 'must',\n",
       " 'support',\n",
       " 'co',\n",
       " '##rt',\n",
       " '##ical',\n",
       " 'com',\n",
       " '##putation',\n",
       " 'in',\n",
       " 'a',\n",
       " 'biological',\n",
       " '##ly',\n",
       " 'realistic',\n",
       " 'fashion',\n",
       " '.',\n",
       " 'Going',\n",
       " 'beyond',\n",
       " 'simply',\n",
       " 'addressing',\n",
       " 'the',\n",
       " 'role',\n",
       " 'of',\n",
       " 'spikes',\n",
       " 'in',\n",
       " 'co',\n",
       " '##rt',\n",
       " '##ical',\n",
       " 'cells',\n",
       " \"'\",\n",
       " 'communication',\n",
       " ',',\n",
       " 'we',\n",
       " 'show',\n",
       " 'that',\n",
       " 'our',\n",
       " 'distributed',\n",
       " 's',\n",
       " '##ync',\n",
       " '##hr',\n",
       " '##ony',\n",
       " 'model',\n",
       " 'can',\n",
       " 'be',\n",
       " 'implemented',\n",
       " 'in',\n",
       " 'a',\n",
       " 'predict',\n",
       " '##ive',\n",
       " 'coding',\n",
       " 'framework',\n",
       " 'and',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'structures',\n",
       " 'in',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'environment',\n",
       " '.',\n",
       " 'Train',\n",
       " '##ed',\n",
       " 'with',\n",
       " 'patches',\n",
       " 'from',\n",
       " 'natural',\n",
       " 'images',\n",
       " ',',\n",
       " 'our',\n",
       " 'model',\n",
       " 'V',\n",
       " '##1',\n",
       " 'cells',\n",
       " 'develop',\n",
       " 'localized',\n",
       " 'and',\n",
       " 'oriented',\n",
       " 're',\n",
       " '##ceptive',\n",
       " 'fields',\n",
       " ',',\n",
       " 'consistent',\n",
       " 'with',\n",
       " 'V',\n",
       " '##1',\n",
       " 'simple',\n",
       " 'cell',\n",
       " 'properties',\n",
       " '.',\n",
       " 'Unlike',\n",
       " 'most',\n",
       " 'co',\n",
       " '##rt',\n",
       " '##ical',\n",
       " 'com',\n",
       " '##putation',\n",
       " 'models',\n",
       " ',',\n",
       " 'our',\n",
       " 'predict',\n",
       " '##ive',\n",
       " 'coding',\n",
       " 'model',\n",
       " 'makes',\n",
       " 'use',\n",
       " 'of',\n",
       " 'single',\n",
       " 'spikes',\n",
       " ',',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'abstract',\n",
       " '##ing',\n",
       " 'spikes',\n",
       " 'away',\n",
       " 'with',\n",
       " 'analog',\n",
       " 'quantities',\n",
       " '.',\n",
       " 'This',\n",
       " 'close',\n",
       " 'resemblance',\n",
       " 'to',\n",
       " 'biology',\n",
       " 'makes',\n",
       " 'our',\n",
       " 'model',\n",
       " 'well',\n",
       " 'suited',\n",
       " 'for',\n",
       " 'guiding',\n",
       " 'experimental',\n",
       " 'research',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_cstr.tokenize(X_train_bal.iloc[0], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer_cstr(examples, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = [dict(tokenize_function(X_train_bal.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_train_bal))]\n",
    "val_ds = [dict(tokenize_function(X_val.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_val))]\n",
    "test_ds = [dict(tokenize_function(X_test.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(cstr_lb.transform(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_cstr = AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=num_labels, torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "logging_step = len(train_ds)//batch_size\n",
    "model_ckpt = str(datetime.datetime.today().date().isoformat())\n",
    "model_name = f\"{model_ckpt}-finetune-cstr\"\n",
    "model_name_final = f\"{model_ckpt}-bert-finetune-cstr-final\"\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_step,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\",\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_cstr,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset=val_ds,\n",
    "                  processing_class=tokenizer_cstr\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86' max='86' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86/86 01:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.742900</td>\n",
       "      <td>0.801921</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.205100</td>\n",
       "      <td>0.827584</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.814614</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=86, training_loss=0.46993461736412934, metrics={'train_runtime': 98.9511, 'train_samples_per_second': 6.872, 'train_steps_per_second': 0.869, 'total_flos': 178918730465280.0, 'train_loss': 0.46993461736412934, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_name_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8,\n",
       " 'f1_macro': np.float64(0.7928741235593271),\n",
       " 'f1_micro': np.float64(0.8)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(trainer.predict(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RE8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "re8 = pd.read_csv('../dados/atv3/re8.csv')\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(re8['text'], re8['class'], test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "re8_lb = LabelEncoder().fit(re8['class'])\n",
    "num_labels = len(re8_lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7674, 3)\n",
      "(5371,)\n",
      "(783,)\n",
      "(1520,)\n"
     ]
    }
   ],
   "source": [
    "print(re8.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "earn        2771\n",
       "money       2771\n",
       "interest    2771\n",
       "ship        2771\n",
       "acq         2771\n",
       "trade       2771\n",
       "crude       2771\n",
       "grain       2771\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bal, y_train_bal = balance_df(X_train, y_train)\n",
    "y_train_bal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_re8 = AutoTokenizer.from_pretrained(model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'un',\n",
       " '##iba',\n",
       " '##nco',\n",
       " '##rp',\n",
       " 'in',\n",
       " '##c',\n",
       " 'u',\n",
       " '##b',\n",
       " '##c',\n",
       " '##p',\n",
       " 'regular',\n",
       " 'divide',\n",
       " '##nd',\n",
       " 'set',\n",
       " 'q',\n",
       " '##tly',\n",
       " 'di',\n",
       " '##v',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'vs',\n",
       " 'c',\n",
       " '##ts',\n",
       " 'previously',\n",
       " 'pay',\n",
       " 'a',\n",
       " '##p',\n",
       " '##ril',\n",
       " 'record',\n",
       " 'march',\n",
       " 're',\n",
       " '##uter',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_re8.tokenize(X_train_bal.iloc[0], add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer_re8(examples, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = [dict(tokenize_function(X_train_bal.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_train_bal))]\n",
    "val_ds = [dict(tokenize_function(X_val.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_val))]\n",
    "test_ds = [dict(tokenize_function(X_test.iloc[i])) | {\"label\": int(c)} for i, c in enumerate(re8_lb.transform(y_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_re8 = AutoModelForSequenceClassification.from_pretrained(model_base, num_labels=num_labels, torch_dtype=\"auto\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "logging_step = len(train_ds)//batch_size\n",
    "model_ckpt = str(datetime.datetime.today().date().isoformat())\n",
    "model_name = f\"{model_ckpt}-finetune-re8\"\n",
    "model_name_final = f\"{model_ckpt}-bert-finetune-re8-final\"\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1_macro = f1_score(labels, preds, average=\"macro\")\n",
    "    f1_micro = f1_score(labels, preds, average=\"micro\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro, \"f1_micro\": f1_micro}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=1e-4,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_step,\n",
    "                                  push_to_hub=False,\n",
    "                                  log_level=\"error\",\n",
    "                                  load_best_model_at_end=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_re8,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=train_ds,\n",
    "                  eval_dataset=val_ds,\n",
    "                  processing_class=tokenizer_re8\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5542' max='5542' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5542/5542 1:32:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.007600</td>\n",
       "      <td>2.043580</td>\n",
       "      <td>0.028097</td>\n",
       "      <td>0.006832</td>\n",
       "      <td>0.028097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.099300</td>\n",
       "      <td>2.101748</td>\n",
       "      <td>0.010217</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5542, training_loss=2.0534499687048897, metrics={'train_runtime': 5547.4652, 'train_samples_per_second': 7.992, 'train_steps_per_second': 0.999, 'total_flos': 1.1665920178126848e+16, 'train_loss': 2.0534499687048897, 'epoch': 2.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_name_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 94/190 00:29 < 00:30, 3.11 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.034210526315789476,\n",
       " 'f1_macro': np.float64(0.00826972010178117),\n",
       " 'f1_micro': np.float64(0.034210526315789476)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(trainer.predict(test_ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
